{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporate Synergy Bot 7B - Training Notebook\n",
    "\n",
    "This notebook trains a LoRA adapter on Mistral-7B for corporate speak transformation.\n",
    "\n",
    "**Important**: Make sure to select GPU runtime (Runtime → Change runtime type → T4 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'nvidia-smi' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2024 NVIDIA Corporation\n",
      "Built on Fri_Jun_14_16:44:19_Pacific_Daylight_Time_2024\n",
      "Cuda compilation tools, release 12.6, V12.6.20\n",
      "Build cuda_12.6.r12.6/compiler.34431801_0\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability and CUDA version first\n",
    "!nvidia-smi\n",
    "!nvcc --version\n",
    "\n",
    "# Install CUDA dependencies for Google Colab\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Install required packages with specific versions\n",
    "!pip install -q transformers==4.36.2\n",
    "!pip install -q datasets==2.14.7\n",
    "!pip install -q peft==0.7.1\n",
    "!pip install -q accelerate==0.25.0\n",
    "!pip install -q bitsandbytes==0.41.3\n",
    "!pip install -q tensorboard\n",
    "!pip install -q huggingface-hub\n",
    "\n",
    "# Verify CUDA installation\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "print(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Corporate Speak Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from enum import Enum\n",
    "from typing import List, Dict, Tuple\n",
    "import json\n",
    "\n",
    "class Domain(Enum):\n",
    "    TECH_STARTUP = \"tech_startup\"\n",
    "    CONSULTING = \"consulting\"\n",
    "    FINANCE = \"finance\"\n",
    "    HEALTHCARE = \"healthcare\"\n",
    "    RETAIL = \"retail\"\n",
    "    MANUFACTURING = \"manufacturing\"\n",
    "\n",
    "class SeniorityLevel(Enum):\n",
    "    JUNIOR = 1\n",
    "    MID = 2\n",
    "    SENIOR = 3\n",
    "    EXECUTIVE = 4\n",
    "\n",
    "# Corporate vocabulary by domain\n",
    "DOMAIN_VOCAB = {\n",
    "    Domain.TECH_STARTUP: {\n",
    "        \"nouns\": [\"MVP\", \"pivot\", \"scale\", \"disruption\", \"iteration\", \"velocity\", \"sprint\", \"backlog\"],\n",
    "        \"verbs\": [\"iterate\", \"pivot\", \"scale\", \"disrupt\", \"innovate\", \"optimize\", \"refactor\"],\n",
    "        \"phrases\": [\"move fast and break things\", \"fail fast\", \"growth hack\", \"10x engineer\"]\n",
    "    },\n",
    "    Domain.CONSULTING: {\n",
    "        \"nouns\": [\"deliverables\", \"framework\", \"methodology\", \"stakeholders\", \"engagement\", \"deck\"],\n",
    "        \"verbs\": [\"leverage\", \"align\", \"synthesize\", \"cascade\", \"socialize\", \"operationalize\"],\n",
    "        \"phrases\": [\"circle back\", \"touch base\", \"deep dive\", \"low-hanging fruit\", \"move the needle\"]\n",
    "    },\n",
    "    Domain.FINANCE: {\n",
    "        \"nouns\": [\"ROI\", \"EBITDA\", \"runway\", \"burn rate\", \"valuation\", \"portfolio\", \"assets\"],\n",
    "        \"verbs\": [\"monetize\", \"capitalize\", \"hedge\", \"diversify\", \"liquidate\", \"optimize returns\"],\n",
    "        \"phrases\": [\"top line growth\", \"bottom line impact\", \"risk-adjusted returns\", \"market dynamics\"]\n",
    "    },\n",
    "    Domain.HEALTHCARE: {\n",
    "        \"nouns\": [\"outcomes\", \"protocols\", \"compliance\", \"wellness\", \"interventions\", \"metrics\"],\n",
    "        \"verbs\": [\"implement\", \"standardize\", \"coordinate\", \"integrate\", \"monitor\", \"assess\"],\n",
    "        \"phrases\": [\"patient-centered care\", \"evidence-based practice\", \"quality metrics\", \"care coordination\"]\n",
    "    },\n",
    "    Domain.RETAIL: {\n",
    "        \"nouns\": [\"touchpoints\", \"omnichannel\", \"conversion\", \"basket size\", \"footfall\", \"inventory\"],\n",
    "        \"verbs\": [\"engage\", \"convert\", \"upsell\", \"personalize\", \"optimize\", \"streamline\"],\n",
    "        \"phrases\": [\"customer journey\", \"seamless experience\", \"drive traffic\", \"enhance engagement\"]\n",
    "    },\n",
    "    Domain.MANUFACTURING: {\n",
    "        \"nouns\": [\"throughput\", \"efficiency\", \"lean\", \"six sigma\", \"capacity\", \"yield\", \"supply chain\"],\n",
    "        \"verbs\": [\"optimize\", \"streamline\", \"scale\", \"automate\", \"standardize\", \"implement\"],\n",
    "        \"phrases\": [\"continuous improvement\", \"operational excellence\", \"just-in-time\", \"quality control\"]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Seniority-based language patterns\n",
    "SENIORITY_PATTERNS = {\n",
    "    SeniorityLevel.JUNIOR: {\n",
    "        \"starters\": [\"I think\", \"Maybe we could\", \"I was wondering if\", \"Would it be possible to\"],\n",
    "        \"confidence\": 0.6,\n",
    "        \"jargon_density\": 0.3\n",
    "    },\n",
    "    SeniorityLevel.MID: {\n",
    "        \"starters\": [\"I recommend\", \"We should consider\", \"Based on my analysis\", \"I suggest\"],\n",
    "        \"confidence\": 0.8,\n",
    "        \"jargon_density\": 0.5\n",
    "    },\n",
    "    SeniorityLevel.SENIOR: {\n",
    "        \"starters\": [\"We need to\", \"Let's\", \"I'm implementing\", \"We're driving\"],\n",
    "        \"confidence\": 0.9,\n",
    "        \"jargon_density\": 0.7\n",
    "    },\n",
    "    SeniorityLevel.EXECUTIVE: {\n",
    "        \"starters\": [\"We will\", \"I'm directing\", \"Our strategy\", \"The vision is\"],\n",
    "        \"confidence\": 1.0,\n",
    "        \"jargon_density\": 0.9\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ Corporate vocabulary and patterns defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create Dataset Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CorporateDatasetGenerator:\n",
    "    def __init__(self):\n",
    "        self.casual_to_corporate = {\n",
    "            # Basic transformations\n",
    "            \"let's meet\": [\"let's sync up\", \"let's align\", \"let's circle back\", \"let's touch base\"],\n",
    "            \"good job\": [\"excellent execution\", \"great deliverables\", \"strong performance\", \"impactful work\"],\n",
    "            \"i need help\": [\"i require support\", \"i need assistance\", \"seeking guidance\", \"require collaboration\"],\n",
    "            \"let's talk\": [\"let's discuss\", \"let's have a dialogue\", \"let's connect\", \"let's interface\"],\n",
    "            \"i'm busy\": [\"my bandwidth is limited\", \"i'm at capacity\", \"my plate is full\", \"i'm resource-constrained\"],\n",
    "            \"works for me\": [\"that aligns with my schedule\", \"i can accommodate that\", \"that's feasible\", \"i'm aligned\"],\n",
    "            \"problem\": [\"challenge\", \"opportunity for improvement\", \"pain point\", \"area of concern\"],\n",
    "            \"fix\": [\"resolve\", \"address\", \"remediate\", \"optimize\"],\n",
    "            \"use\": [\"leverage\", \"utilize\", \"employ\", \"harness\"],\n",
    "            \"think about\": [\"consider\", \"evaluate\", \"assess\", \"analyze\"],\n",
    "            \"work together\": [\"collaborate\", \"synergize\", \"partner\", \"align efforts\"],\n",
    "            \"improve\": [\"optimize\", \"enhance\", \"elevate\", \"drive improvement\"],\n",
    "            \"start\": [\"initiate\", \"commence\", \"kick off\", \"launch\"],\n",
    "            \"end\": [\"conclude\", \"finalize\", \"wrap up\", \"bring to closure\"],\n",
    "            \"make\": [\"create\", \"develop\", \"produce\", \"generate\"],\n",
    "            \"check\": [\"validate\", \"verify\", \"assess\", \"review\"],\n",
    "            \"send\": [\"distribute\", \"disseminate\", \"cascade\", \"share\"],\n",
    "            \"get\": [\"obtain\", \"acquire\", \"secure\", \"procure\"],\n",
    "            \"show\": [\"demonstrate\", \"illustrate\", \"present\", \"showcase\"],\n",
    "            \"tell\": [\"communicate\", \"inform\", \"advise\", \"brief\"]\n",
    "        }\n",
    "        \n",
    "        # Reverse mappings for bidirectional translation\n",
    "        self.corporate_to_casual = {}\n",
    "        for casual, corporate_list in self.casual_to_corporate.items():\n",
    "            for corporate in corporate_list:\n",
    "                self.corporate_to_casual[corporate] = casual\n",
    "        \n",
    "        # Additional corporate phrases\n",
    "        self.corporate_phrases = [\n",
    "            \"moving forward\", \"going forward\", \"at the end of the day\", \"net net\",\n",
    "            \"circle back\", \"touch base\", \"reach out\", \"loop in\", \"take this offline\",\n",
    "            \"bandwidth\", \"capacity\", \"deliverables\", \"action items\", \"key takeaways\",\n",
    "            \"synergies\", \"best practices\", \"lessons learned\", \"pain points\", \"win-win\",\n",
    "            \"low-hanging fruit\", \"move the needle\", \"paradigm shift\", \"game changer\",\n",
    "            \"think outside the box\", \"30,000 foot view\", \"drill down\", \"deep dive\",\n",
    "            \"leverage our learnings\", \"operationalize\", \"productize\", \"incentivize\"\n",
    "        ]\n",
    "    \n",
    "    def generate_examples(self, num_examples: int = 1000) -> List[Dict]:\n",
    "        examples = []\n",
    "        \n",
    "        # Generate different types of examples\n",
    "        for i in range(num_examples):\n",
    "            example_type = random.choice([\n",
    "                \"casual_to_corporate\",\n",
    "                \"corporate_to_casual\",\n",
    "                \"domain_specific\",\n",
    "                \"seniority_based\",\n",
    "                \"conversation\",\n",
    "                \"email\",\n",
    "                \"meeting\"\n",
    "            ])\n",
    "            \n",
    "            if example_type == \"casual_to_corporate\":\n",
    "                example = self._generate_casual_to_corporate()\n",
    "            elif example_type == \"corporate_to_casual\":\n",
    "                example = self._generate_corporate_to_casual()\n",
    "            elif example_type == \"domain_specific\":\n",
    "                example = self._generate_domain_specific()\n",
    "            elif example_type == \"seniority_based\":\n",
    "                example = self._generate_seniority_based()\n",
    "            elif example_type == \"conversation\":\n",
    "                example = self._generate_conversation()\n",
    "            elif example_type == \"email\":\n",
    "                example = self._generate_email()\n",
    "            else:\n",
    "                example = self._generate_meeting()\n",
    "            \n",
    "            # Add the formatted text field\n",
    "            example['text'] = f\"### Instruction: {example['instruction']}\\n### Input: {example['input']}\\n### Response: {example['output']}\"\n",
    "            examples.append(example)\n",
    "        \n",
    "        return examples\n",
    "    \n",
    "    def _generate_casual_to_corporate(self) -> Dict:\n",
    "        casual_phrase = random.choice(list(self.casual_to_corporate.keys()))\n",
    "        corporate_phrase = random.choice(self.casual_to_corporate[casual_phrase])\n",
    "        \n",
    "        # Create variations\n",
    "        templates = [\n",
    "            (f\"I think we should {casual_phrase}\", f\"I believe we should {corporate_phrase}\"),\n",
    "            (f\"Can we {casual_phrase}?\", f\"Could we {corporate_phrase}?\"),\n",
    "            (f\"Let's {casual_phrase} about this\", f\"Let's {corporate_phrase} regarding this matter\"),\n",
    "            (f\"We need to {casual_phrase}\", f\"We need to {corporate_phrase}\")\n",
    "        ]\n",
    "        \n",
    "        input_text, output_text = random.choice(templates)\n",
    "        \n",
    "        return {\n",
    "            \"instruction\": \"Transform to corporate speak\",\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        }\n",
    "    \n",
    "    def _generate_corporate_to_casual(self) -> Dict:\n",
    "        corporate_phrase = random.choice(list(self.corporate_to_casual.keys()))\n",
    "        casual_phrase = self.corporate_to_casual[corporate_phrase]\n",
    "        \n",
    "        templates = [\n",
    "            (f\"We need to {corporate_phrase} on this initiative\", f\"We need to {casual_phrase} on this\"),\n",
    "            (f\"Let's {corporate_phrase} to ensure alignment\", f\"Let's {casual_phrase}\"),\n",
    "            (f\"I'll {corporate_phrase} with the team\", f\"I'll {casual_phrase} with the team\")\n",
    "        ]\n",
    "        \n",
    "        input_text, output_text = random.choice(templates)\n",
    "        \n",
    "        return {\n",
    "            \"instruction\": \"Translate corporate speak to plain English\",\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text\n",
    "        }\n",
    "    \n",
    "    def _generate_domain_specific(self) -> Dict:\n",
    "        domain = random.choice(list(Domain))\n",
    "        vocab = DOMAIN_VOCAB[domain]\n",
    "        \n",
    "        noun = random.choice(vocab[\"nouns\"])\n",
    "        verb = random.choice(vocab[\"verbs\"])\n",
    "        phrase = random.choice(vocab[\"phrases\"])\n",
    "        \n",
    "        templates = [\n",
    "            (\"how's the project?\", f\"We're {verb}ing our {noun} to {phrase}\"),\n",
    "            (\"what's the plan?\", f\"The plan is to {verb} the {noun} and {phrase}\"),\n",
    "            (\"any updates?\", f\"Yes, we've been able to {phrase} by {verb}ing our {noun}\")\n",
    "        ]\n",
    "        \n",
    "        input_text, output_text = random.choice(templates)\n",
    "        \n",
    "        return {\n",
    "            \"instruction\": f\"Transform to {domain.value.replace('_', ' ')} corporate speak\",\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text,\n",
    "            \"context\": {\"domain\": domain.value}\n",
    "        }\n",
    "    \n",
    "    def _generate_seniority_based(self) -> Dict:\n",
    "        seniority = random.choice(list(SeniorityLevel))\n",
    "        patterns = SENIORITY_PATTERNS[seniority]\n",
    "        starter = random.choice(patterns[\"starters\"])\n",
    "        \n",
    "        casual_inputs = [\n",
    "            \"we should try something new\",\n",
    "            \"this isn't working\",\n",
    "            \"i have an idea\",\n",
    "            \"let's change this\"\n",
    "        ]\n",
    "        \n",
    "        input_text = random.choice(casual_inputs)\n",
    "        \n",
    "        # Generate output based on seniority\n",
    "        if seniority == SeniorityLevel.JUNIOR:\n",
    "            output_text = f\"{starter} we could explore alternative approaches\"\n",
    "        elif seniority == SeniorityLevel.MID:\n",
    "            output_text = f\"{starter} pivoting our strategy to drive better outcomes\"\n",
    "        elif seniority == SeniorityLevel.SENIOR:\n",
    "            output_text = f\"{starter} implementing a strategic pivot to optimize results\"\n",
    "        else:  # EXECUTIVE\n",
    "            output_text = f\"{starter} to transform our approach and maximize value creation\"\n",
    "        \n",
    "        return {\n",
    "            \"instruction\": f\"Transform to corporate speak (seniority: {seniority.name})\",\n",
    "            \"input\": input_text,\n",
    "            \"output\": output_text,\n",
    "            \"context\": {\"seniority\": seniority.value}\n",
    "        }\n",
    "    \n",
    "    def _generate_conversation(self) -> Dict:\n",
    "        scenarios = [\n",
    "            {\n",
    "                \"input\": \"thanks for your help\",\n",
    "                \"output\": \"I appreciate your collaboration on this initiative. Your contributions have been invaluable.\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"sorry i'm late\",\n",
    "                \"output\": \"Apologies for the delay. I was addressing another priority that required immediate attention.\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"can you explain this?\",\n",
    "                \"output\": \"I'd be happy to provide additional context and clarification on this matter.\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"i don't understand\",\n",
    "                \"output\": \"Let me break this down further to ensure we're aligned on the key concepts.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        scenario = random.choice(scenarios)\n",
    "        \n",
    "        return {\n",
    "            \"instruction\": \"Generate a professional response\",\n",
    "            \"input\": scenario[\"input\"],\n",
    "            \"output\": scenario[\"output\"],\n",
    "            \"context\": {\"type\": \"conversation\"}\n",
    "        }\n",
    "    \n",
    "    def _generate_email(self) -> Dict:\n",
    "        email_types = [\n",
    "            {\n",
    "                \"input\": \"need this done today\",\n",
    "                \"output\": \"This deliverable has been identified as a critical priority with an EOD deadline. Please advise on your capacity to accommodate this urgent request.\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"following up on my last email\",\n",
    "                \"output\": \"I wanted to circle back on my previous communication to ensure alignment and address any outstanding questions or concerns.\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"fyi\",\n",
    "                \"output\": \"Please find below information for your awareness and consideration.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        email_type = random.choice(email_types)\n",
    "        \n",
    "        return {\n",
    "            \"instruction\": \"Transform to professional email language\",\n",
    "            \"input\": email_type[\"input\"],\n",
    "            \"output\": email_type[\"output\"],\n",
    "            \"context\": {\"type\": \"email\"}\n",
    "        }\n",
    "    \n",
    "    def _generate_meeting(self) -> Dict:\n",
    "        meeting_phrases = [\n",
    "            {\n",
    "                \"input\": \"let's start\",\n",
    "                \"output\": \"Let's kick off today's session. Thank you all for joining.\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"any questions?\",\n",
    "                \"output\": \"Are there any questions, concerns, or additional perspectives to consider?\"\n",
    "            },\n",
    "            {\n",
    "                \"input\": \"that's all\",\n",
    "                \"output\": \"That concludes our agenda items. Thank you for your valuable contributions and engagement.\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        phrase = random.choice(meeting_phrases)\n",
    "        \n",
    "        return {\n",
    "            \"instruction\": \"Transform to professional meeting language\",\n",
    "            \"input\": phrase[\"input\"],\n",
    "            \"output\": phrase[\"output\"],\n",
    "            \"context\": {\"type\": \"meeting\"}\n",
    "        }\n",
    "\n",
    "# Create generator instance\n",
    "generator = CorporateDatasetGenerator()\n",
    "print(\"✅ Dataset generator created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generate Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Generate examples\n",
    "print(\"Generating dataset...\")\n",
    "num_examples = 8000  # Adjust based on your needs\n",
    "all_examples = generator.generate_examples(num_examples)\n",
    "\n",
    "# Convert to DataFrame for easy splitting\n",
    "df = pd.DataFrame(all_examples)\n",
    "\n",
    "# Split into train/validation/test (80/10/10)\n",
    "train_size = int(0.8 * len(df))\n",
    "val_size = int(0.1 * len(df))\n",
    "\n",
    "train_df = df[:train_size]\n",
    "val_df = df[train_size:train_size + val_size]\n",
    "test_df = df[train_size + val_size:]\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "dataset = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"validation\": Dataset.from_pandas(val_df),\n",
    "    \"test\": Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "print(f\"\\n✅ Dataset generated!\")\n",
    "print(f\"Train: {len(dataset['train'])} examples\")\n",
    "print(f\"Validation: {len(dataset['validation'])} examples\")\n",
    "print(f\"Test: {len(dataset['test'])} examples\")\n",
    "\n",
    "# Show some examples\n",
    "print(\"\\n📝 Sample examples:\")\n",
    "for i in range(3):\n",
    "    example = dataset['train'][i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Instruction: {example['instruction']}\")\n",
    "    print(f\"Input: {example['input']}\")\n",
    "    print(f\"Output: {example['output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face - you'll need to enter your token\n",
    "print(\"Please enter your Hugging Face token:\")\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Model name\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"✅ Tokenizer loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"\\n✅ LoRA configuration applied!\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"✅ Dataset tokenized successfully!\")\n",
    "print(f\"Training examples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(tokenized_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./corporate-synergy-bot-7b\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_steps=25,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"phxdev/corporate-synergy-bot-7b\",\n",
    ")\n",
    "\n",
    "print(\"✅ Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer created and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"🚀 Starting training...\")\n",
    "print(\"This will take approximately 2-3 hours on a T4 GPU\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save and Push Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model locally\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model()\n",
    "print(\"✅ Model saved locally!\")\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "print(\"\\nPushing to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(\"phxdev/corporate-synergy-bot-7b\")\n",
    "\n",
    "print(\"\\n🎉 Model successfully pushed to: https://huggingface.co/phxdev/corporate-synergy-bot-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Test examples\n",
    "test_cases = [\n",
    "    \"### Instruction: Transform to corporate speak\\n### Input: let's meet tomorrow\\n### Response:\",\n",
    "    \"### Instruction: Transform to corporate speak\\n### Input: I need help\\n### Response:\",\n",
    "    \"### Instruction: Translate corporate speak to plain English\\n### Input: We need to leverage our synergies\\n### Response:\",\n",
    "    \"### Instruction: Transform to tech corporate speak (seniority: senior)\\n### Input: good job on the project\\n### Response:\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing the model...\\n\")\n",
    "for test in test_cases:\n",
    "    print(f\"Input: {test.split('### Input: ')[1].split('### Response:')[0].strip()}\")\n",
    "    print(f\"Output: {generate_response(test)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "Your Corporate Synergy Bot 7B has been trained and uploaded to Hugging Face!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Check your model at: https://huggingface.co/phxdev/corporate-synergy-bot-7b\n",
    "2. Create a demo Space using the `app.py` file\n",
    "3. Share your bot with the community!\n",
    "\n",
    "Remember: To maximize stakeholder value, we must leverage our synergies through collaborative paradigm shifts! 😄"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "extras",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
