{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corporate Synergy Bot 7B - Training Notebook\n",
    "\n",
    "This notebook trains a LoRA adapter on Mistral-7B for corporate speak transformation.\n",
    "\n",
    "**Important**: Make sure to select GPU runtime (Runtime → Change runtime type → T4 GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Check GPU availability and CUDA version first\n!nvidia-smi\n!nvcc --version\n\n# Install CUDA dependencies for Google Colab\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n# Install required packages with specific versions\n!pip install -q transformers==4.36.2\n!pip install -q datasets==2.14.7\n!pip install -q peft==0.7.1\n!pip install -q accelerate==0.25.0\n!pip install -q bitsandbytes==0.41.3\n!pip install -q tensorboard\n!pip install -q huggingface-hub\n\n# Verify CUDA installation\nimport torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA version: {torch.version.cuda}\")\nprint(f\"Device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "!nvidia-smi\n",
    "\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Login to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Login to Hugging Face - you'll need to enter your token\n",
    "print(\"Please enter your Hugging Face token:\")\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Dataset with Fixed Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# Try to load the dataset\n",
    "try:\n",
    "    dataset = load_dataset(\"phxdev/corporate-speak-dataset\")\n",
    "    print(\"✅ Dataset loaded successfully!\")\n",
    "    print(f\"Dataset structure: {dataset}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")\n",
    "    print(\"Creating dataset from scratch...\")\n",
    "    \n",
    "    # If loading fails, create a simple dataset\n",
    "    data = {\n",
    "        \"text\": [\n",
    "            \"### Instruction: Transform to corporate speak\\n### Input: let's meet tomorrow\\n### Response: Let's sync up tomorrow to align on our objectives\",\n",
    "            \"### Instruction: Transform to corporate speak\\n### Input: good job\\n### Response: Excellent execution on those deliverables\",\n",
    "            \"### Instruction: Translate corporate speak to plain English\\n### Input: We need to leverage our synergies\\n### Response: We need to work together\",\n",
    "        ] * 1000  # Repeat for training\n",
    "    }\n",
    "    \n",
    "    # Create train/validation splits\n",
    "    df = pd.DataFrame(data)\n",
    "    train_df = df[:800]\n",
    "    val_df = df[800:]\n",
    "    \n",
    "    dataset = DatasetDict({\n",
    "        \"train\": Dataset.from_pandas(train_df),\n",
    "        \"validation\": Dataset.from_pandas(val_df)\n",
    "    })\n",
    "\n",
    "# Display first example\n",
    "print(\"\\nFirst training example:\")\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Model name\n",
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Load tokenizer\n",
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"✅ Tokenizer loaded: {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Model with 4-bit Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure 4-bit quantization\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# Load model\n",
    "print(\"Loading model with 4-bit quantization...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"✅ Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Configure LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "print(\"Preparing model for LoRA training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(\"\\n✅ LoRA configuration applied!\")\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # Ensure we're working with the text column\n",
    "    if 'text' in examples:\n",
    "        texts = examples['text']\n",
    "    else:\n",
    "        # Fallback: create text from other columns if needed\n",
    "        texts = [f\"### Instruction: {inst}\\n### Input: {inp}\\n### Response: {out}\" \n",
    "                 for inst, inp, out in zip(examples.get('instruction', ['']*len(examples)), \n",
    "                                          examples.get('input', ['']*len(examples)), \n",
    "                                          examples.get('output', ['']*len(examples)))]\n",
    "    \n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "print(\"Tokenizing dataset...\")\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "print(\"✅ Dataset tokenized successfully!\")\n",
    "print(f\"Training examples: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation examples: {len(tokenized_dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Setup Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./corporate-synergy-bot-7b\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    gradient_checkpointing=True,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    weight_decay=0.001,\n",
    "    fp16=True,\n",
    "    bf16=False,\n",
    "    max_grad_norm=0.3,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.1,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    report_to=\"tensorboard\",\n",
    "    logging_steps=25,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    save_total_limit=3,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"phxdev/corporate-synergy-bot-7b\",\n",
    ")\n",
    "\n",
    "print(\"✅ Training arguments configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "print(\"✅ Trainer created and ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"🚀 Starting training...\")\n",
    "print(\"This will take approximately 2-3 hours on a T4 GPU\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n✅ Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save and Push Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model locally\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model()\n",
    "print(\"✅ Model saved locally!\")\n",
    "\n",
    "# Push to Hugging Face Hub\n",
    "print(\"\\nPushing to Hugging Face Hub...\")\n",
    "trainer.push_to_hub()\n",
    "tokenizer.push_to_hub(\"phxdev/corporate-synergy-bot-7b\")\n",
    "\n",
    "print(\"\\n🎉 Model successfully pushed to: https://huggingface.co/phxdev/corporate-synergy-bot-7b\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Test the Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=150):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_length,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    # Extract just the response part\n",
    "    if \"### Response:\" in response:\n",
    "        response = response.split(\"### Response:\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "# Test examples\n",
    "test_cases = [\n",
    "    \"### Instruction: Transform to corporate speak\\n### Input: let's meet tomorrow\\n### Response:\",\n",
    "    \"### Instruction: Transform to corporate speak\\n### Input: I need help\\n### Response:\",\n",
    "    \"### Instruction: Translate corporate speak to plain English\\n### Input: We need to leverage our synergies\\n### Response:\",\n",
    "    \"### Instruction: Transform to tech corporate speak (seniority: senior)\\n### Input: good job on the project\\n### Response:\"\n",
    "]\n",
    "\n",
    "print(\"🧪 Testing the model...\\n\")\n",
    "for test in test_cases:\n",
    "    print(f\"Input: {test.split('### Input: ')[1].split('### Response:')[0].strip()}\")\n",
    "    print(f\"Output: {generate_response(test)}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Create Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_card = \"\"\"---\n",
    "license: apache-2.0\n",
    "base_model: mistralai/Mistral-7B-Instruct-v0.2\n",
    "tags:\n",
    "- generated_from_trainer\n",
    "- text-generation\n",
    "- conversational\n",
    "- corporate-speak\n",
    "datasets:\n",
    "- phxdev/corporate-speak-dataset\n",
    "language:\n",
    "- en\n",
    "---\n",
    "\n",
    "# Corporate Synergy Bot 7B\n",
    "\n",
    "This model transforms casual language into professional corporate communication and vice versa.\n",
    "\n",
    "## Model Details\n",
    "\n",
    "- **Base Model**: Mistral-7B-Instruct-v0.2\n",
    "- **Training**: LoRA fine-tuning\n",
    "- **Parameters**: r=16, alpha=32\n",
    "- **Dataset**: 7,953 bidirectional examples\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model = PeftModel.from_pretrained(model, \"phxdev/corporate-synergy-bot-7b\")\n",
    "\n",
    "prompt = \"### Instruction: Transform to corporate speak\\\\n### Input: let's meet\\\\n### Response:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "```\n",
    "\n",
    "## Examples\n",
    "\n",
    "**Casual → Corporate:**\n",
    "- \"let's meet\" → \"Let's sync up to align on our objectives\"\n",
    "- \"good job\" → \"Excellent execution on those deliverables\"\n",
    "\n",
    "**Corporate → Casual:**\n",
    "- \"We need to leverage our synergies\" → \"We need to work together\"\n",
    "\"\"\"\n",
    "\n",
    "# Save model card\n",
    "with open(\"README.md\", \"w\") as f:\n",
    "    f.write(model_card)\n",
    "\n",
    "print(\"✅ Model card created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎉 Congratulations!\n",
    "\n",
    "Your Corporate Synergy Bot 7B has been trained and uploaded to Hugging Face!\n",
    "\n",
    "**Next Steps:**\n",
    "1. Check your model at: https://huggingface.co/phxdev/corporate-synergy-bot-7b\n",
    "2. Create a demo Space using the `app.py` file in the demo folder\n",
    "3. Share your bot with the community!\n",
    "\n",
    "Remember: To maximize stakeholder value, we must leverage our synergies through collaborative paradigm shifts! 😄"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}